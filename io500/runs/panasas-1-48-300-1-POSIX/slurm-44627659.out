SLURM_JOB_ID: 44627659
dimensions: {"nodes": 1, "ntasks_per_node": 48, "filesystem": {"name": "panasas", "mountpoint": "/gws/pw/j07/perf_testing3"}, "stonewall": 300, "iters": 1, "time": "4:0:0", "ior_api": "POSIX"}
SLURM_JOB_NODELIST: host823
INFO:    Using cached SIF image
INFO:    Using cached SIF image
INFO:    Using cached SIF image
INFO:    Using cached SIF image
INFO:    Using cached SIF image
INFO:    Using cached SIF image
INFO:    Using cached SIF image
INFO:    Using cached SIF image
INFO:    Using cached SIF image
INFO:    Using cached SIF image
INFO:    Using cached SIF image
INFO:    Using cached SIF image
INFO:    Using cached SIF image
INFO:    Using cached SIF image
INFO:    Using cached SIF image
INFO:    Using cached SIF image
INFO:    Using cached SIF image
INFO:    Using cached SIF image
INFO:    Using cached SIF image
INFO:    Using cached SIF image
INFO:    Using cached SIF image
INFO:    Using cached SIF image
INFO:    Using cached SIF image
INFO:    Using cached SIF image
INFO:    Using cached SIF image
INFO:    Using cached SIF image
INFO:    Using cached SIF image
INFO:    Using cached SIF image
INFO:    Using cached SIF image
INFO:    Using cached SIF image
INFO:    Using cached SIF image
INFO:    Using cached SIF image
INFO:    Using cached SIF image
INFO:    Using cached SIF image
INFO:    Using cached SIF image
INFO:    Using cached SIF image
INFO:    Using cached SIF image
INFO:    Using cached SIF image
INFO:    Using cached SIF image
INFO:    Using cached SIF image
INFO:    Using cached SIF image
INFO:    Using cached SIF image
INFO:    Using cached SIF image
INFO:    Using cached SIF image
INFO:    Using cached SIF image
INFO:    Using cached SIF image
INFO:    Using cached SIF image
INFO:    Using cached SIF image
[host823.jc.rl.ac.uk:38503] OPAL ERROR: Unreachable in file ext2x_client.c at line 112
*** An error occurred in MPI_Init
*** on a NULL communicator
*** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
***    and potentially your MPI job)
[host823.jc.rl.ac.uk:38503] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!
--------------------------------------------------------------------------
Primary job  terminated normally, but 1 process returned
a non-zero exit code. Per user-direction, the job has been aborted.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
mpirun detected that one or more processes exited with non-zero status, thus causing
the job to be terminated. The first process to do so was:

  Process name: [[6221,1],22]
  Exit code:    1
--------------------------------------------------------------------------
slurmstepd: error: *** JOB 44627659 ON host823 CANCELLED AT 2023-03-09T21:07:56 DUE TO TIME LIMIT ***
